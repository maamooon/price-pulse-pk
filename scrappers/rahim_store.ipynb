{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2YGgjp9l-B0f"
      },
      "outputs": [],
      "source": [
        "import asyncio\n",
        "from playwright.async_api import async_playwright\n",
        "import csv\n",
        "import os\n",
        "from typing import List, Dict, Optional\n",
        "import logging\n",
        "from datetime import datetime\n",
        "import time\n",
        "import sys\n",
        "\n",
        "# Set up detailed logging with real-time output\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s | %(levelname)-8s | %(message)s',\n",
        "    datefmt='%H:%M:%S',\n",
        "    handlers=[\n",
        "        logging.FileHandler('rahim_scraper_detailed.log', encoding='utf-8'),\n",
        "        logging.StreamHandler(sys.stdout)\n",
        "    ]\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class RahimStoreScraper:\n",
        "    def __init__(self):\n",
        "        self.base_url = \"https://www.rahimstore.com/department/\"\n",
        "        self.departments = ['001', '002', '003', '004', '005', '006']\n",
        "        self.output_file = f'rahim_store_products_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.csv'\n",
        "        self.product_data = []\n",
        "        self.stats = {\n",
        "            'total_products': 0,\n",
        "            'failed_products': 0,\n",
        "            'department_stats': {},\n",
        "            'start_time': None,\n",
        "            'end_time': None,\n",
        "            'current_department': None,\n",
        "            'current_page': 0\n",
        "        }\n",
        "\n",
        "    def print_banner(self):\n",
        "        \"\"\"Print startup banner\"\"\"\n",
        "        print(\"\\n\" + \"â•\" * 80)\n",
        "        print(\"RAHIM STORE WEB SCRAPER - REAL-TIME MONITORING\")\n",
        "        print(\"â•\" * 80)\n",
        "        print(f\"Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "        print(f\"Output file: {self.output_file}\")\n",
        "        print(f\"Departments: {', '.join(self.departments)}\")\n",
        "        print(\"â•\" * 80)\n",
        "        print(\"LIVE LOGS STARTING...\")\n",
        "        print(\"â”€\" * 80)\n",
        "\n",
        "    def print_live_status(self, action: str, details: str = \"\"):\n",
        "        \"\"\"Print real-time status updates\"\"\"\n",
        "        timestamp = datetime.now().strftime('%H:%M:%S')\n",
        "        print(f\"[{timestamp}] ðŸ”„ {action}: {details}\")\n",
        "\n",
        "    def print_product_extracted(self, product_name: str, product_id: str, price: str):\n",
        "        \"\"\"Print when a product is successfully extracted\"\"\"\n",
        "        timestamp = datetime.now().strftime('%H:%M:%S')\n",
        "        print(f\"[{timestamp}] EXTRACTED: '{product_name}' (ID: {product_id}) - Price: {price}\")\n",
        "\n",
        "    def print_page_progress(self, dept: str, page: int, current: int, total: int):\n",
        "        \"\"\"Print page-by-page progress\"\"\"\n",
        "        progress = f\"Dept {dept} | Page {page} | Progress: {current}/{total} products\"\n",
        "        percentage = (current / total) * 100 if total > 0 else 0\n",
        "        print(f\"[{dept}-P{page}] â–ˆ{'â–ˆ' * int(percentage/10)}{'â–‘' * (10 - int(percentage/10))}â”‚ {current}/{total} ({percentage:.1f}%)\")\n",
        "\n",
        "    def print_department_summary(self, dept: str, total_products: int, duration: float):\n",
        "        \"\"\"Print department completion summary\"\"\"\n",
        "        print(\"â”€\" * 80)\n",
        "        print(f\"DEPARTMENT {dept} COMPLETED!\")\n",
        "        print(f\"Total products: {total_products}\")\n",
        "        print(f\"Time taken: {duration:.2f} seconds\")\n",
        "        print(\"â”€\" * 80)\n",
        "\n",
        "    def print_final_summary(self):\n",
        "        \"\"\"Print final summary\"\"\"\n",
        "        total_duration = self.stats['end_time'] - self.stats['start_time']\n",
        "\n",
        "        print(\"\\n\" + \"â•\" * 80)\n",
        "        print(\"SCRAPING COMPLETED - FINAL SUMMARY\")\n",
        "        print(\"â•\" * 80)\n",
        "        print(f\"Total duration: {total_duration:.2f} seconds\")\n",
        "        print(f\"Total products: {self.stats['total_products']}\")\n",
        "        print(f\"Failed extractions: {self.stats['failed_products']}\")\n",
        "        print(f\"Output file: {self.output_file}\")\n",
        "\n",
        "        print(\"\\nDEPARTMENT BREAKDOWN:\")\n",
        "        for dept in self.departments:\n",
        "            count = self.stats['department_stats'].get(dept, 0)\n",
        "            print(f\"   â””â”€â”€ Department {dept}: {count} products\")\n",
        "\n",
        "        total_attempts = self.stats['total_products'] + self.stats['failed_products']\n",
        "        success_rate = (self.stats['total_products'] / total_attempts * 100) if total_attempts > 0 else 0\n",
        "        print(f\"\\nSuccess rate: {success_rate:.1f}%\")\n",
        "        print(\"â•\" * 80)\n",
        "\n",
        "    async def setup_browser(self) -> bool:\n",
        "        \"\"\"Initialize browser and context\"\"\"\n",
        "        try:\n",
        "            self.print_live_status(\"INITIALIZING BROWSER\", \"Starting Playwright...\")\n",
        "            self.stats['start_time'] = time.time()\n",
        "\n",
        "            self.playwright = await async_playwright().start()\n",
        "            self.browser = await self.playwright.chromium.launch(\n",
        "                headless=True,\n",
        "                args=[\n",
        "                    '--no-sandbox',\n",
        "                    '--disable-dev-shm-usage',\n",
        "                    '--disable-blink-features=AutomationControlled',\n",
        "                    '--disable-features=VizDisplayCompositor'\n",
        "                ]\n",
        "            )\n",
        "\n",
        "            self.context = await self.browser.new_context(\n",
        "                viewport={'width': 1920, 'height': 1080},\n",
        "                user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
        "                extra_http_headers={\n",
        "                    'Accept-Language': 'en-US,en;q=0.9',\n",
        "                    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'\n",
        "                }\n",
        "            )\n",
        "\n",
        "            self.print_live_status(\"BROWSER READY\", \"Chromium initialized successfully\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            self.print_live_status(\"BROWSER FAILED\", f\"Error: {str(e)}\")\n",
        "            return False\n",
        "\n",
        "    async def close_browser(self):\n",
        "        \"\"\"Close browser resources\"\"\"\n",
        "        try:\n",
        "            if hasattr(self, 'context'):\n",
        "                await self.context.close()\n",
        "            if hasattr(self, 'browser'):\n",
        "                await self.browser.close()\n",
        "            if hasattr(self, 'playwright'):\n",
        "                await self.playwright.stop()\n",
        "            self.print_live_status(\"BROWSER CLOSED\", \"All resources cleaned up\")\n",
        "        except Exception as e:\n",
        "            self.print_live_status(\"CLEANUP ERROR\", f\"Error: {str(e)}\")\n",
        "\n",
        "    def save_to_csv(self) -> bool:\n",
        "        \"\"\"Save data to CSV file\"\"\"\n",
        "        try:\n",
        "            if not self.product_data:\n",
        "                self.print_live_status(\"CSV SAVE\", \"No data to save\")\n",
        "                return False\n",
        "\n",
        "            fieldnames = [\n",
        "                'department_id', 'product_id', 'product_name', 'current_price',\n",
        "                'original_price', 'product_url', 'image_url', 'was_price',\n",
        "                'scraped_timestamp'\n",
        "            ]\n",
        "\n",
        "            with open(self.output_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "                writer.writeheader()\n",
        "                writer.writerows(self.product_data)\n",
        "\n",
        "            self.print_live_status(\"CSV SAVED\", f\"{len(self.product_data)} products saved to {self.output_file}\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            self.print_live_status(\"CSV ERROR\", f\"Failed to save: {str(e)}\")\n",
        "            return False\n",
        "\n",
        "    async def wait_for_products_fully_loaded(self, page, department_id: str, page_number: int):\n",
        "        \"\"\"Wait for products to be fully loaded with multiple verification steps\"\"\"\n",
        "        try:\n",
        "            self.print_live_status(\"WAITING\", f\"Waiting for products to fully load on page {page_number}\")\n",
        "\n",
        "            # Wait for the main product container to be present\n",
        "            await page.wait_for_selector('.item.img-hover-zoom--quick-zoom', timeout=20000)\n",
        "            self.print_live_status(\"LOAD CHECK\", \"Initial product container found\")\n",
        "\n",
        "            # Wait for additional time to ensure all content is loaded\n",
        "            await asyncio.sleep(2)\n",
        "\n",
        "            # Check if product images are loaded\n",
        "            images_loaded = await page.query_selector_all('img.img-fluid[src]')\n",
        "            self.print_live_status(\"LOAD CHECK\", f\"{len(images_loaded)} product images found\")\n",
        "\n",
        "            # Wait for prices to be loaded\n",
        "            prices_loaded = await page.query_selector_all('strong')\n",
        "            self.print_live_status(\"LOAD CHECK\", f\"{len(prices_loaded)} price elements found\")\n",
        "\n",
        "            # Final wait to ensure everything is rendered\n",
        "            await asyncio.sleep(1)\n",
        "\n",
        "            self.print_live_status(\"LOAD COMPLETE\", \"All products fully loaded and ready for scraping\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            self.print_live_status(\"LOAD ERROR\", f\"Failed to wait for products: {str(e)}\")\n",
        "            return False\n",
        "\n",
        "    async def extract_product_info(self, product_card, department_id: str, card_index: int) -> Optional[Dict]:\n",
        "        \"\"\"Extract detailed product information with real-time logging\"\"\"\n",
        "        try:\n",
        "            # Extract product name\n",
        "            name_element = await product_card.query_selector('a[style=\"display:block; height:50px;\"]')\n",
        "            if not name_element:\n",
        "                self.print_live_status(\"EXTRACTION FAILED\", f\"Card {card_index}: No name element found\")\n",
        "                return None\n",
        "\n",
        "            product_name = await name_element.inner_text()\n",
        "            product_name = product_name.strip() if product_name else \"N/A\"\n",
        "\n",
        "            # Extract product URL and ID\n",
        "            product_url = await name_element.get_attribute('href') or \"N/A\"\n",
        "            product_id = await name_element.get_attribute('productid') or \"N/A\"\n",
        "\n",
        "            # Extract image\n",
        "            img_element = await product_card.query_selector('img.img-fluid')\n",
        "            image_url = await img_element.get_attribute('src') if img_element else \"N/A\"\n",
        "\n",
        "            # Extract prices\n",
        "            strong_element = await product_card.query_selector('strong')\n",
        "            current_price = await strong_element.inner_text() if strong_element else \"N/A\"\n",
        "            current_price = current_price.replace('Rs', '').replace('sup', '').strip()\n",
        "\n",
        "            strike_element = await product_card.query_selector('strike')\n",
        "            was_price = await strike_element.inner_text() if strike_element else \"N/A\"\n",
        "\n",
        "            # Get additional price data from button\n",
        "            button_element = await product_card.query_selector('button.btn-success')\n",
        "            original_price = was_price\n",
        "\n",
        "            if button_element:\n",
        "                button_data = await button_element.get_attribute('data')\n",
        "                if button_data and '~' in button_data:\n",
        "                    data_parts = button_data.split('~')\n",
        "                    if len(data_parts) >= 4:\n",
        "                        original_price = data_parts[3]\n",
        "\n",
        "            product_info = {\n",
        "                'department_id': department_id,\n",
        "                'product_id': product_id,\n",
        "                'product_name': product_name,\n",
        "                'current_price': current_price,\n",
        "                'original_price': original_price,\n",
        "                'product_url': product_url,\n",
        "                'image_url': image_url,\n",
        "                'was_price': was_price,\n",
        "                'scraped_timestamp': datetime.now().isoformat()\n",
        "            }\n",
        "\n",
        "            # Print successful extraction\n",
        "            self.print_product_extracted(product_name, product_id, current_price)\n",
        "            return product_info\n",
        "\n",
        "        except Exception as e:\n",
        "            self.print_live_status(\"EXTRACTION ERROR\", f\"Card {card_index}: {str(e)}\")\n",
        "            self.stats['failed_products'] += 1\n",
        "            return None\n",
        "\n",
        "    async def handle_pagination(self, page, department_id: str, current_page: int) -> bool:\n",
        "        \"\"\"Handle pagination to next page with proper waiting\"\"\"\n",
        "        try:\n",
        "            self.print_live_status(\"PAGINATION\", f\"Checking for page {current_page + 1}\")\n",
        "\n",
        "            # Wait for pagination to load\n",
        "            await page.wait_for_selector('.pagination', timeout=10000)\n",
        "\n",
        "            # Find next button\n",
        "            next_button = await page.query_selector('a.page-link[aria-label=\"Next\"]')\n",
        "            if not next_button:\n",
        "                self.print_live_status(\"PAGINATION\", \"No next button found\")\n",
        "                return False\n",
        "\n",
        "            # Check if next button is disabled\n",
        "            is_disabled = await next_button.evaluate('(element) => element.parentElement.classList.contains(\"disabled\")')\n",
        "            if is_disabled:\n",
        "                self.print_live_status(\"PAGINATION\", f\"Reached last page ({current_page})\")\n",
        "                return False\n",
        "\n",
        "            # Click next button\n",
        "            self.print_live_status(\"PAGINATION\", f\"Moving to page {current_page + 1}\")\n",
        "            await next_button.click()\n",
        "\n",
        "            # Wait for navigation to complete and new page to load\n",
        "            self.print_live_status(\"WAITING\", \"Waiting for new page to load after pagination...\")\n",
        "            await page.wait_for_timeout(4000)  # Increased wait time for page transition\n",
        "\n",
        "            # Wait for products to be fully loaded on the new page\n",
        "            await self.wait_for_products_fully_loaded(page, department_id, current_page + 1)\n",
        "\n",
        "            self.print_live_status(\"PAGINATION\", f\"Successfully loaded page {current_page + 1}\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            self.print_live_status(\"PAGINATION ERROR\", f\"Page {current_page}: {str(e)}\")\n",
        "            return False\n",
        "\n",
        "    async def scrape_department_page(self, page, department_id: str, page_number: int) -> bool:\n",
        "        \"\"\"Scrape a single page of products with proper loading waits\"\"\"\n",
        "        try:\n",
        "            self.stats['current_page'] = page_number\n",
        "            self.print_live_status(\"PAGE START\", f\"Department {department_id} - Page {page_number}\")\n",
        "\n",
        "            # Wait for products to be fully loaded before scraping\n",
        "            if not await self.wait_for_products_fully_loaded(page, department_id, page_number):\n",
        "                self.print_live_status(\"PAGE ERROR\", f\"Products not loaded properly on page {page_number}\")\n",
        "                return False\n",
        "\n",
        "            # Get all product cards after ensuring they're loaded\n",
        "            product_cards = await page.query_selector_all('.item.img-hover-zoom--quick-zoom')\n",
        "\n",
        "            if not product_cards:\n",
        "                self.print_live_status(\"PAGE EMPTY\", \"No product cards found after waiting\")\n",
        "                return False\n",
        "\n",
        "            self.print_live_status(\"PRODUCTS FOUND\", f\"Found {len(product_cards)} products on page {page_number}\")\n",
        "\n",
        "            # Extract each product with small delays between extractions\n",
        "            successful_extractions = 0\n",
        "            for i, card in enumerate(product_cards, 1):\n",
        "                # Show progress every 5 products\n",
        "                if i % 5 == 0 or i == len(product_cards):\n",
        "                    self.print_page_progress(department_id, page_number, i, len(product_cards))\n",
        "\n",
        "                # Small delay between product extractions to be respectful\n",
        "                if i > 1:\n",
        "                    await asyncio.sleep(0.1)\n",
        "\n",
        "                product_info = await self.extract_product_info(card, department_id, i)\n",
        "                if product_info:\n",
        "                    self.product_data.append(product_info)\n",
        "                    successful_extractions += 1\n",
        "                    self.stats['total_products'] += 1\n",
        "\n",
        "            self.print_live_status(\"PAGE COMPLETE\",\n",
        "                f\"Page {page_number}: {successful_extractions}/{len(product_cards)} products extracted\")\n",
        "\n",
        "            # Handle pagination\n",
        "            has_next_page = await self.handle_pagination(page, department_id, page_number)\n",
        "            return has_next_page\n",
        "\n",
        "        except Exception as e:\n",
        "            self.print_live_status(\"PAGE ERROR\", f\"Department {department_id} Page {page_number}: {str(e)}\")\n",
        "            return False\n",
        "\n",
        "    async def scrape_department(self, department_id: str):\n",
        "        \"\"\"Scrape all pages of a department with proper loading waits\"\"\"\n",
        "        dept_start_time = time.time()\n",
        "        url = f\"{self.base_url}{department_id}\"\n",
        "\n",
        "        self.stats['current_department'] = department_id\n",
        "        print(f\"\\n{'=' * 80}\")\n",
        "        print(f\"STARTING DEPARTMENT {department_id}\")\n",
        "        print(f\"URL: {url}\")\n",
        "        print(f\"{'=' * 80}\")\n",
        "\n",
        "        page = await self.context.new_page()\n",
        "        dept_products_start = len(self.product_data)\n",
        "\n",
        "        try:\n",
        "            # Navigate to department with longer timeout\n",
        "            self.print_live_status(\"NAVIGATING\", f\"Loading {url} (waiting for full load)...\")\n",
        "            response = await page.goto(url, wait_until='domcontentloaded', timeout=45000)\n",
        "\n",
        "            if not response or response.status != 200:\n",
        "                status_code = getattr(response, 'status', 'Unknown')\n",
        "                self.print_live_status(\"NAVIGATION FAILED\", f\"HTTP {status_code}\")\n",
        "                return\n",
        "\n",
        "            self.print_live_status(\"PAGE LOADED\", \"Initial page loaded, waiting for full content...\")\n",
        "\n",
        "            # Wait for the department page to be fully ready\n",
        "            await self.wait_for_products_fully_loaded(page, department_id, 1)\n",
        "\n",
        "            # Scrape all pages\n",
        "            page_number = 1\n",
        "            max_pages = 100  # Safety limit\n",
        "\n",
        "            while page_number <= max_pages:\n",
        "                has_next_page = await self.scrape_department_page(page, department_id, page_number)\n",
        "\n",
        "                if not has_next_page:\n",
        "                    self.print_live_status(\"DEPARTMENT COMPLETE\", f\"No more pages after page {page_number}\")\n",
        "                    break\n",
        "\n",
        "                page_number += 1\n",
        "                # Increased pause between pages\n",
        "                await asyncio.sleep(2)\n",
        "\n",
        "            # Department summary\n",
        "            dept_products_count = len(self.product_data) - dept_products_start\n",
        "            self.stats['department_stats'][department_id] = dept_products_count\n",
        "            dept_duration = time.time() - dept_start_time\n",
        "\n",
        "            self.print_department_summary(department_id, dept_products_count, dept_duration)\n",
        "\n",
        "        except Exception as e:\n",
        "            self.print_live_status(\"DEPARTMENT ERROR\", f\"Department {department_id}: {str(e)}\")\n",
        "        finally:\n",
        "            await page.close()\n",
        "            self.print_live_status(\"PAGE CLOSED\", f\"Department {department_id} browser page closed\")\n",
        "\n",
        "    async def run_scraper(self):\n",
        "        \"\"\"Main scraper execution function\"\"\"\n",
        "        self.print_banner()\n",
        "\n",
        "        if not await self.setup_browser():\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            # Scrape each department\n",
        "            for i, department_id in enumerate(self.departments, 1):\n",
        "                print(f\"\\nOVERALL PROGRESS: Department {i}/{len(self.departments)}\")\n",
        "                await self.scrape_department(department_id)\n",
        "\n",
        "                # Longer pause between departments to be respectful\n",
        "                if i < len(self.departments):\n",
        "                    self.print_live_status(\"PAUSING\", \"Waiting 5 seconds before next department...\")\n",
        "                    await asyncio.sleep(5)\n",
        "\n",
        "            # Final save and summary\n",
        "            self.stats['end_time'] = time.time()\n",
        "            self.save_to_csv()\n",
        "            self.print_final_summary()\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            self.print_live_status(\"INTERRUPTED\", \"User stopped the scraper\")\n",
        "            print(\"\\nSaving collected data before exit...\")\n",
        "            self.save_to_csv()\n",
        "        except Exception as e:\n",
        "            self.print_live_status(\"FATAL ERROR\", f\"Scraper crashed: {str(e)}\")\n",
        "        finally:\n",
        "            await self.close_browser()\n",
        "\n",
        "async def main():\n",
        "    \"\"\"Main execution function\"\"\"\n",
        "    print(\"Initializing Rahim Store Scraper with Real-time Monitoring...\")\n",
        "\n",
        "    try:\n",
        "        scraper = RahimStoreScraper()\n",
        "        await scraper.run_scraper()\n",
        "    except Exception as e:\n",
        "        print(f\"Fatal initialization error: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Run the scraper\n",
        "    await main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install playwright\n",
        "!playwright install"
      ],
      "metadata": {
        "id": "ew8kteYc_MmA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}