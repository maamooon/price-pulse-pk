{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GrocerApp Scraper\n",
    "This notebook scrapes product data from GrocerApp Pakistan using Playwright for infinite scroll support.\n",
    "\n",
    "### Instructions:\n",
    "1. Run the **Setup Cell** to install dependencies and Playwright browser.\n",
    "2. Run the **Scraper Logic Cell** to define the classes.\n",
    "3. Run the **Execution Cell** to start scraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Setup Cell\n",
    "!pip install playwright nest_asyncio pandas\n",
    "!playwright install chromium\n",
    "import os\n",
    "os.makedirs('data', exist_ok=True)\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Base Infrastructure\n",
    "import csv\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional\n",
    "from abc import ABC, abstractmethod\n",
    "import time\n",
    "import random\n",
    "\n",
    "class BaseScraper(ABC):\n",
    "    def __init__(self, store_name: str, base_url: str, output_dir: str = \"data\"):\n",
    "        self.store_name = store_name\n",
    "        self.base_url = base_url\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.output_dir.mkdir(exist_ok=True)\n",
    "        self.logger = self._setup_logger()\n",
    "        self.csv_headers = [\n",
    "            'store_name', 'product_name', 'brand', 'category', 'subcategory',\n",
    "            'price', 'discounted_price', 'unit', 'quantity', 'url', 'image_url', 'last_updated'\n",
    "        ]\n",
    "    \n",
    "    def _setup_logger(self) -> logging.Logger:\n",
    "        logger = logging.getLogger(f\"{self.store_name}_scraper\")\n",
    "        logger.setLevel(logging.INFO)\n",
    "        if not logger.handlers:\n",
    "            ch = logging.StreamHandler()\n",
    "            ch.setLevel(logging.INFO)\n",
    "            formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "            ch.setFormatter(formatter)\n",
    "            logger.addHandler(ch)\n",
    "        return logger\n",
    "    \n",
    "    def _rate_limit(self, min_delay: float = 1.0, max_delay: float = 3.0):\n",
    "        time.sleep(random.uniform(min_delay, max_delay))\n",
    "    \n",
    "    def _clean_price(self, price_str: str) -> Optional[float]:\n",
    "        if not price_str: return None\n",
    "        try:\n",
    "            cleaned = str(price_str).replace('Rs.', '').replace('PKR', '').replace('Rs', '').replace(',', '').strip()\n",
    "            return float(cleaned)\n",
    "        except (ValueError, AttributeError): return None\n",
    "    \n",
    "    def _parse_unit_quantity(self, text: str) -> tuple[Optional[str], Optional[float]]:\n",
    "        if not text: return None, None\n",
    "        import re\n",
    "        patterns = [r'(\\d+\\.?\\d*)\\s*(kg|g|l|ml|piece|pcs|pack)', r'(\\d+\\.?\\d*)(kg|g|l|ml|piece|pcs|pack)']\n",
    "        for pattern in patterns:\n",
    "            match = re.search(pattern, text.lower())\n",
    "            if match: return match.group(2), float(match.group(1))\n",
    "        return None, None\n",
    "    \n",
    "    def save_to_csv(self, products: List[Dict], filename: Optional[str] = None):\n",
    "        if not products: return\n",
    "        if filename is None:\n",
    "            filename = f\"{self.store_name.lower().replace(' ', '_')}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "        filepath = self.output_dir / filename\n",
    "        with open(filepath, 'w', newline='', encoding='utf-8-sig') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=self.csv_headers)\n",
    "            writer.writeheader()\n",
    "            for product in products:\n",
    "                row = {header: product.get(header, None) for header in self.csv_headers}\n",
    "                writer.writerow(row)\n",
    "        print(f\"\\u2713 Saved {len(products)} products to {filepath}\")\n",
    "\n",
    "    def validate_product(self, product: Dict) -> bool: return bool(product.get('product_name') and product.get('price'))\n",
    "    \n",
    "    @abstractmethod\n",
    "    async def scrape(self) -> List[Dict]: pass\n",
    "    @abstractmethod\n",
    "    async def get_categories(self) -> List[Dict]: pass\n",
    "    \n",
    "    def create_product_dict(self, **kwargs) -> Dict:\n",
    "        d = {h: kwargs.get(h) for h in self.csv_headers}\n",
    "        d['store_name'] = self.store_name\n",
    "        d['last_updated'] = datetime.now().isoformat()\n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title GrocerApp Scraper Logic\n",
    "import asyncio\n",
    "from playwright.async_api import async_playwright\n",
    "\n",
    "class GrocerAppScraper(BaseScraper):\n",
    "    def __init__(self, output_dir: str = \"data\"):\n",
    "        super().__init__(store_name=\"GrocerApp\", base_url=\"https://grocerapp.pk\", output_dir=output_dir)\n",
    "        self.playwright = None\n",
    "        self.browser = None\n",
    "        self.context = None\n",
    "    \n",
    "    async def setup_browser(self):\n",
    "        self.playwright = await async_playwright().start()\n",
    "        self.browser = await self.playwright.chromium.launch(headless=True, args=['--no-sandbox', '--disable-dev-shm-usage'])\n",
    "        self.context = await self.browser.new_context(viewport={'width': 1920, 'height': 1080}, user_agent='Mozilla/5.0')\n",
    "    \n",
    "    async def close_browser(self):\n",
    "        if self.context: await self.context.close()\n",
    "        if self.browser: await self.browser.close()\n",
    "        if self.playwright: await self.playwright.stop()\n",
    "    \n",
    "    async def scroll_to_bottom(self, page):\n",
    "        last_height = await page.evaluate(\"document.body.scrollHeight\")\n",
    "        while True:\n",
    "            await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\")\n",
    "            await asyncio.sleep(3)\n",
    "            new_height = await page.evaluate(\"document.body.scrollHeight\")\n",
    "            if new_height == last_height: break\n",
    "            last_height = new_height\n",
    "            if await page.locator(\".MuiCard-root\").count() > 300: break\n",
    "            \n",
    "    async def get_categories(self) -> List[Dict]:\n",
    "        page = await self.context.new_page()\n",
    "        try:\n",
    "            await page.goto(f\"{self.base_url}/categories\", wait_until=\"networkidle\", timeout=60000)\n",
    "            await asyncio.sleep(2)\n",
    "            categories = await page.evaluate('''() => {\n",
    "                const cats = [];\n",
    "                document.querySelectorAll('a[href]').forEach(a => {\n",
    "                    const nameEl = a.querySelector('p');\n",
    "                    const name = nameEl ? nameEl.innerText.trim() : '';\n",
    "                    const path = a.getAttribute('href');\n",
    "                    if (name && path && path.length > 1 && !path.includes('categories') && !path.includes('cart') && !path.includes('login')) {\n",
    "                        if (!cats.find(c => c.url === a.href)) cats.push({ name, url: a.href });\n",
    "                    }\n",
    "                });\n",
    "                return cats;\n",
    "            }''')\n",
    "            return categories\n",
    "        finally: await page.close()\n",
    "\n",
    "    async def scrape_category(self, category: Dict) -> List[Dict]:\n",
    "        page = await self.context.new_page()\n",
    "        products = []\n",
    "        try:\n",
    "            await page.goto(category['url'], wait_until=\"networkidle\", timeout=60000)\n",
    "            await self.scroll_to_bottom(page)\n",
    "            cards = await page.evaluate('''() => {\n",
    "                const res = [];\n",
    "                document.querySelectorAll('.MuiCard-root').forEach(card => {\n",
    "                    const nameEl = card.querySelector('.MuiTypography-body1');\n",
    "                    const priceEl = card.querySelector('.MuiTypography-subtitle2');\n",
    "                    const imgEl = card.querySelector('img');\n",
    "                    const linkEl = card.querySelector('a');\n",
    "                    if (nameEl && priceEl) res.push({\n",
    "                        name: nameEl.innerText.trim(),\n",
    "                        priceText: priceEl.innerText.trim(),\n",
    "                        img: imgEl ? imgEl.src : null,\n",
    "                        url: linkEl ? linkEl.href : null\n",
    "                    });\n",
    "                });\n",
    "                return res;\n",
    "            }''')\n",
    "            for card in cards:\n",
    "                unit, quantity = self._parse_unit_quantity(card['name'])\n",
    "                product = self.create_product_dict(\n",
    "                    product_name=card['name'], price=self._clean_price(card['priceText']),\n",
    "                    url=card['url'], image_url=card['img'], category=category['name'],\n",
    "                    unit=unit, quantity=quantity\n",
    "                )\n",
    "                if self.validate_product(product): products.append(product)\n",
    "            return products\n",
    "        finally: await page.close()\n",
    "\n",
    "    async def scrape(self) -> List[Dict]:\n",
    "        all_products = []\n",
    "        await self.setup_browser()\n",
    "        try:\n",
    "            categories = await self.get_categories()\n",
    "            for category in categories[:10]: # Limit categories for testing\n",
    "                products = await self.scrape_category(category)\n",
    "                all_products.extend(products)\n",
    "                await asyncio.sleep(1)\n",
    "        finally: await self.close_browser()\n",
    "        return all_products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Execution\n",
    "scraper = GrocerAppScraper()\n",
    "products = await scraper.scrape()\n",
    "if products:\n",
    "    scraper.save_to_csv(products)\n",
    "import pandas as pd\n",
    "if products:\n",
    "    df = pd.DataFrame(products)\n",
    "    display(df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
