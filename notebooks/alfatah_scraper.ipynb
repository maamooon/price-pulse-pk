{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Al-Fatah Scraper\n",
    "This notebook scrapes product data from Al-Fatah Pakistan using their Shopify JSON API.\n",
    "\n",
    "### Instructions:\n",
    "1. Run the **Setup Cell** to install dependencies.\n",
    "2. Run the **Scraper Logic Cell** to define the classes.\n",
    "3. Run the **Execution Cell** to start scraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Setup Cell\n",
    "!pip install aiohttp pandas\n",
    "import os\n",
    "os.makedirs('data', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Base Infrastructure\n",
    "import csv\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional\n",
    "from abc import ABC, abstractmethod\n",
    "import time\n",
    "import random\n",
    "\n",
    "class BaseScraper(ABC):\n",
    "    def __init__(self, store_name: str, base_url: str, output_dir: str = \"data\"):\n",
    "        self.store_name = store_name\n",
    "        self.base_url = base_url\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.output_dir.mkdir(exist_ok=True)\n",
    "        self.logger = self._setup_logger()\n",
    "        self.csv_headers = [\n",
    "            'store_name', 'product_name', 'brand', 'category', 'subcategory',\n",
    "            'price', 'discounted_price', 'unit', 'quantity', 'url', 'image_url', 'last_updated'\n",
    "        ]\n",
    "    \n",
    "    def _setup_logger(self) -> logging.Logger:\n",
    "        logger = logging.getLogger(f\"{self.store_name}_scraper\")\n",
    "        logger.setLevel(logging.INFO)\n",
    "        if not logger.handlers:\n",
    "            ch = logging.StreamHandler()\n",
    "            ch.setLevel(logging.INFO)\n",
    "            formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "            ch.setFormatter(formatter)\n",
    "            logger.addHandler(ch)\n",
    "        return logger\n",
    "    \n",
    "    def _rate_limit(self, min_delay: float = 1.0, max_delay: float = 3.0):\n",
    "        time.sleep(random.uniform(min_delay, max_delay))\n",
    "    \n",
    "    def _clean_price(self, price_str: str) -> Optional[float]:\n",
    "        if not price_str: return None\n",
    "        try:\n",
    "            cleaned = str(price_str).replace('Rs.', '').replace('PKR', '').replace('Rs', '').replace(',', '').strip()\n",
    "            return float(cleaned)\n",
    "        except (ValueError, AttributeError): return None\n",
    "    \n",
    "    def _parse_unit_quantity(self, text: str) -> tuple[Optional[str], Optional[float]]:\n",
    "        if not text: return None, None\n",
    "        import re\n",
    "        patterns = [r'(\\d+\\.?\\d*)\\s*(kg|g|l|ml|piece|pcs|pack)', r'(\\d+\\.?\\d*)(kg|g|l|ml|piece|pcs|pack)']\n",
    "        for pattern in patterns:\n",
    "            match = re.search(pattern, text.lower())\n",
    "            if match: return match.group(2), float(match.group(1))\n",
    "        return None, None\n",
    "    \n",
    "    def save_to_csv(self, products: List[Dict], filename: Optional[str] = None):\n",
    "        if not products: return\n",
    "        if filename is None:\n",
    "            filename = f\"{self.store_name.lower().replace(' ', '_')}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "        filepath = self.output_dir / filename\n",
    "        with open(filepath, 'w', newline='', encoding='utf-8-sig') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=self.csv_headers)\n",
    "            writer.writeheader()\n",
    "            for product in products:\n",
    "                row = {header: product.get(header, None) for header in self.csv_headers}\n",
    "                writer.writerow(row)\n",
    "        print(f\"\\u2713 Saved {len(products)} products to {filepath}\")\n",
    "\n",
    "    def validate_product(self, product: Dict) -> bool: return bool(product.get('product_name') and product.get('price'))\n",
    "    \n",
    "    @abstractmethod\n",
    "    async def scrape(self) -> List[Dict]: pass\n",
    "    @abstractmethod\n",
    "    async def get_categories(self) -> List[Dict]: pass\n",
    "    \n",
    "    def create_product_dict(self, **kwargs) -> Dict:\n",
    "        d = {h: kwargs.get(h) for h in self.csv_headers}\n",
    "        d['store_name'] = self.store_name\n",
    "        d['last_updated'] = datetime.now().isoformat()\n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Al-Fatah Scraper Logic\n",
    "import asyncio\n",
    "import aiohttp\n",
    "\n",
    "class AlFatahScraper(BaseScraper):\n",
    "    def __init__(self, output_dir: str = \"data\"):\n",
    "        super().__init__(store_name=\"Al-Fatah\", base_url=\"https://alfatah.pk\", output_dir=output_dir)\n",
    "    \n",
    "    async def fetch_page(self, session: aiohttp.ClientSession, page: int) -> List[Dict]:\n",
    "        url = f\"{self.base_url}/collections/all/products.json\"\n",
    "        try:\n",
    "            async with session.get(url, params={'limit': 250, 'page': page}, timeout=30) as response:\n",
    "                if response.status != 200: return []\n",
    "                data = await response.json()\n",
    "                return data.get('products', [])\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error fetching page {page}: {e}\")\n",
    "            return []\n",
    "\n",
    "    async def get_categories(self) -> List[Dict]:\n",
    "        return [{'name': 'All Products', 'url': f\"{self.base_url}/collections/all\"}]\n",
    "\n",
    "    async def scrape(self) -> List[Dict]:\n",
    "        all_products = []\n",
    "        page = 1\n",
    "        async with aiohttp.ClientSession(headers={'User-Agent': 'Mozilla/5.0'}) as session:\n",
    "            while True:\n",
    "                products = await self.fetch_page(session, page)\n",
    "                if not products: break\n",
    "                for item in products:\n",
    "                    variants = item.get('variants', [])\n",
    "                    if not variants: continue\n",
    "                    v = variants[0]\n",
    "                    unit, quantity = self._parse_unit_quantity(item.get('title', ''))\n",
    "                    product = self.create_product_dict(\n",
    "                        product_name=item.get('title'),\n",
    "                        price=self._clean_price(v.get('price')),\n",
    "                        discounted_price=self._clean_price(v.get('compare_at_price')),\n",
    "                        url=f\"{self.base_url}/products/{item.get('handle')}\",\n",
    "                        image_url=item.get('images', [{}])[0].get('src'),\n",
    "                        brand=item.get('vendor'),\n",
    "                        category=item.get('product_type', 'General'),\n",
    "                        unit=unit, quantity=quantity\n",
    "                    )\n",
    "                    if self.validate_product(product): all_products.append(product)\n",
    "                self.logger.info(f\"\\u2713 Page {page}: Extracted {len(products)} products\")\n",
    "                page += 1\n",
    "                if page > 100: break\n",
    "                await asyncio.sleep(1)\n",
    "        return all_products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Execution\n",
    "scraper = AlFatahScraper()\n",
    "products = await scraper.scrape()\n",
    "if products:\n",
    "    scraper.save_to_csv(products)\n",
    "import pandas as pd\n",
    "if products:\n",
    "    df = pd.DataFrame(products)\n",
    "    display(df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
