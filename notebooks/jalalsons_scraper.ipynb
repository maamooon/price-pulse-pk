{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jalalsons Scraper\n",
    "This notebook scrapes product data from Jalalsons Pakistan using Playwright with multi-branch support.\n",
    "\n",
    "### Instructions:\n",
    "1. Run the **Setup Cell** to install dependencies and Playwright browser.\n",
    "2. Run the **Scraper Logic Cell** to define the classes.\n",
    "3. Run the **Execution Cell** to start scraping. (Note: Scraping all branches can be slow; you can specify a target branch in the execution cell)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Setup Cell\n",
    "!pip install playwright nest_asyncio pandas\n",
    "!playwright install chromium\n",
    "import os\n",
    "os.makedirs('data', exist_ok=True)\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Base Infrastructure\n",
    "import csv\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional\n",
    "from abc import ABC, abstractmethod\n",
    "import time\n",
    "import random\n",
    "\n",
    "class BaseScraper(ABC):\n",
    "    def __init__(self, store_name: str, base_url: str, output_dir: str = \"data\"):\n",
    "        self.store_name = store_name\n",
    "        self.base_url = base_url\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.output_dir.mkdir(exist_ok=True)\n",
    "        self.logger = self._setup_logger()\n",
    "        self.csv_headers = [\n",
    "            'store_name', 'product_name', 'brand', 'category', 'subcategory',\n",
    "            'price', 'discounted_price', 'unit', 'quantity', 'url', 'image_url', 'last_updated'\n",
    "        ]\n",
    "    \n",
    "    def _setup_logger(self) -> logging.Logger:\n",
    "        logger = logging.getLogger(f\"{self.store_name}_scraper\")\n",
    "        logger.setLevel(logging.INFO)\n",
    "        if not logger.handlers:\n",
    "            ch = logging.StreamHandler()\n",
    "            ch.setLevel(logging.INFO)\n",
    "            formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "            ch.setFormatter(formatter)\n",
    "            logger.addHandler(ch)\n",
    "        return logger\n",
    "    \n",
    "    def _rate_limit(self, min_delay: float = 1.0, max_delay: float = 3.0):\n",
    "        time.sleep(random.uniform(min_delay, max_delay))\n",
    "    \n",
    "    def _clean_price(self, price_str: str) -> Optional[float]:\n",
    "        if not price_str: return None\n",
    "        try:\n",
    "            cleaned = str(price_str).replace('Rs.', '').replace('PKR', '').replace('Rs', '').replace(',', '').strip()\n",
    "            return float(cleaned)\n",
    "        except (ValueError, AttributeError): return None\n",
    "    \n",
    "    def _parse_unit_quantity(self, text: str) -> tuple[Optional[str], Optional[float]]:\n",
    "        if not text: return None, None\n",
    "        import re\n",
    "        patterns = [r'(\\d+\\.?\\d*)\\s*(kg|g|l|ml|piece|pcs|pack)', r'(\\d+\\.?\\d*)(kg|g|l|ml|piece|pcs|pack)']\n",
    "        for pattern in patterns:\n",
    "            match = re.search(pattern, text.lower())\n",
    "            if match: return match.group(2), float(match.group(1))\n",
    "        return None, None\n",
    "    \n",
    "    def save_to_csv(self, products: List[Dict], filename: Optional[str] = None):\n",
    "        if not products: return\n",
    "        if filename is None:\n",
    "            filename = f\"{self.store_name.lower().replace(' ', '_')}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "        filepath = self.output_dir / filename\n",
    "        with open(filepath, 'w', newline='', encoding='utf-8-sig') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=self.csv_headers)\n",
    "            writer.writeheader()\n",
    "            for product in products:\n",
    "                row = {header: product.get(header, None) for header in self.csv_headers}\n",
    "                writer.writerow(row)\n",
    "        print(f\"\\u2713 Saved {len(products)} products to {filepath}\")\n",
    "\n",
    "    def validate_product(self, product: Dict) -> bool: return bool(product.get('product_name') and product.get('price'))\n",
    "    \n",
    "    @abstractmethod\n",
    "    async def scrape(self) -> List[Dict]: pass\n",
    "    @abstractmethod\n",
    "    async def get_categories(self) -> List[Dict]: pass\n",
    "    \n",
    "    def create_product_dict(self, **kwargs) -> Dict:\n",
    "        d = {h: kwargs.get(h) for h in self.csv_headers}\n",
    "        d['store_name'] = self.store_name\n",
    "        d['last_updated'] = datetime.now().isoformat()\n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Jalalsons Scraper Logic\n",
    "import asyncio\n",
    "from playwright.async_api import async_playwright\n",
    "\n",
    "class JalalsonsScraper(BaseScraper):\n",
    "    def __init__(self, output_dir: str = \"data\", target_branch: Optional[str] = None):\n",
    "        super().__init__(store_name=\"Jalalsons\", base_url=\"https://jalalsons.com.pk\", output_dir=output_dir)\n",
    "        self.target_branch = target_branch\n",
    "        self.playwright = None\n",
    "        self.browser = None\n",
    "        self.page = None\n",
    "\n",
    "    async def setup_browser(self):\n",
    "        self.playwright = await async_playwright().start()\n",
    "        self.browser = await self.playwright.chromium.launch(headless=True, args=['--no-sandbox', '--disable-dev-shm-usage'])\n",
    "        self.page = await self.browser.new_page()\n",
    "\n",
    "    async def close_browser(self):\n",
    "        if self.page: await self.page.close()\n",
    "        if self.browser: await self.browser.close()\n",
    "        if self.playwright: await self.playwright.stop()\n",
    "\n",
    "    async def close_popup(self):\n",
    "        try:\n",
    "            if await self.page.locator(\"#website_custom_popup\").is_visible():\n",
    "                await self.page.locator('#website_custom_popup .modal-header a.cursor-pointer.ms-auto').click()\n",
    "        except: pass\n",
    "\n",
    "    async def get_branches(self) -> List[str]:\n",
    "        await self.page.goto(self.base_url)\n",
    "        await self.close_popup()\n",
    "        await self.page.click(\"a#delivery-loc-tab\")\n",
    "        await self.page.wait_for_selector(\"#selectDeliveryBranch\")\n",
    "        branches = await self.page.locator(\"#selectDeliveryBranch option\").all_text_contents()\n",
    "        return [b for b in branches if \"Please select\" not in b and \"Lahore\" in b]\n",
    "\n",
    "    async def select_branch(self, branch_name: str, is_first: bool = False):\n",
    "        if not is_first:\n",
    "            await self.page.click(\"a#get_current_loc\")\n",
    "            await self.page.wait_for_selector(\"#selectDeliveryBranch\")\n",
    "        await self.page.select_option(\"#selectDeliveryBranch\", label=branch_name)\n",
    "        await asyncio.sleep(2)\n",
    "        await self.page.click(\"a#delivery_order\")\n",
    "        await self.page.wait_for_selector(\"ul.navbar-nav\")\n",
    "\n",
    "    async def get_categories(self) -> List[Dict]:\n",
    "        nav_items = await self.page.locator(\"ul.navbar-nav > li.nav-item\").element_handles()\n",
    "        results = []\n",
    "        target = [\"BAKERY\", \"DELI\", \"JS ICECREAM\", \"SWEETS\", \"DEALS\", \"GROCERY\"]\n",
    "        for li in nav_items[:-3]:\n",
    "            a = await li.query_selector(\"a.nav-link\")\n",
    "            name = (await a.inner_text()).strip()\n",
    "            if name not in target: continue\n",
    "            await a.hover()\n",
    "            await asyncio.sleep(0.5)\n",
    "            subs = await li.query_selector_all(\"ul.dropdown-content a\")\n",
    "            if subs:\n",
    "                for s in subs: results.append({\"main_category\": name, \"subcategory\": (await s.inner_text()).strip(), \"url\": self.base_url + await s.get_attribute(\"href\")})\n",
    "            else:\n",
    "                results.append({\"main_category\": name, \"subcategory\": name, \"url\": self.base_url + await a.get_attribute(\"href\")})\n",
    "        return results\n",
    "\n",
    "    async def scrape_category(self, category: Dict, branch: str) -> List[Dict]:\n",
    "        products = []\n",
    "        try:\n",
    "            await self.page.goto(category['url'], timeout=60000)\n",
    "            await self.page.wait_for_selector(\".single_product_theme\", timeout=10000)\n",
    "            els = await self.page.query_selector_all(\".single_product_theme\")\n",
    "            for el in els:\n",
    "                n = await el.query_selector(\"p.product_name_theme\")\n",
    "                p_val = await el.query_selector(\"span.price-value\")\n",
    "                i = await el.query_selector(\"img\")\n",
    "                if n and p_val:\n",
    "                    u, q = self._parse_unit_quantity(await n.inner_text())\n",
    "                    prod = self.create_product_dict(product_name=f\"{await n.inner_text()} [{branch}]\", price=self._clean_price(await p_val.inner_text()), url=category['url'], image_url=await i.get_attribute(\"src\"), category=category['main_category'], subcategory=category['subcategory'], unit=u, quantity=q)\n",
    "                    if self.validate_product(prod): products.append(prod)\n",
    "            return products\n",
    "        except: return []\n",
    "\n",
    "    async def scrape(self) -> List[Dict]:\n",
    "        all_p = []\n",
    "        await self.setup_browser()\n",
    "        try:\n",
    "            branches = await self.get_branches()\n",
    "            if self.target_branch: branches = [b for b in branches if self.target_branch.lower() in b.lower()]\n",
    "            for i, b in enumerate(branches[:1]): # Limit 1 branch for demo\n",
    "                await self.select_branch(b, i==0)\n",
    "                cats = await self.get_categories()\n",
    "                for cat in cats[:5]: # Limit categories for demo\n",
    "                    all_p.extend(await self.scrape_category(cat, b))\n",
    "                    await asyncio.sleep(1)\n",
    "        finally: await self.close_browser()\n",
    "        return all_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Execution\n",
    "# You can set target_branch=\"DHA\" to filter\n",
    "scraper = JalalsonsScraper(target_branch=None)\n",
    "products = await scraper.scrape()\n",
    "if products:\n",
    "    scraper.save_to_csv(products)\n",
    "import pandas as pd\n",
    "if products:\n",
    "    df = pd.DataFrame(products)\n",
    "    display(df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
