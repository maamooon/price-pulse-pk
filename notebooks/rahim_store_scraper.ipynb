{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rahim Store Scraper\n",
    "This notebook scrapes product data from Rahim Store Pakistan using Playwright with department-based pagination.\n",
    "\n",
    "### Instructions:\n",
    "1. Run the **Setup Cell** to install dependencies and Playwright browser.\n",
    "2. Run the **Scraper Logic Cell** to define the classes.\n",
    "3. Run the **Execution Cell** to start scraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Setup Cell\n",
    "!pip install playwright nest_asyncio pandas\n",
    "!playwright install chromium\n",
    "import os\n",
    "os.makedirs('data', exist_ok=True)\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Base Infrastructure\n",
    "import csv\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional\n",
    "from abc import ABC, abstractmethod\n",
    "import time\n",
    "import random\n",
    "\n",
    "class BaseScraper(ABC):\n",
    "    def __init__(self, store_name: str, base_url: str, output_dir: str = \"data\"):\n",
    "        self.store_name = store_name\n",
    "        self.base_url = base_url\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.output_dir.mkdir(exist_ok=True)\n",
    "        self.logger = self._setup_logger()\n",
    "        self.csv_headers = [\n",
    "            'store_name', 'product_name', 'brand', 'category', 'subcategory',\n",
    "            'price', 'discounted_price', 'unit', 'quantity', 'url', 'image_url', 'last_updated'\n",
    "        ]\n",
    "    \n",
    "    def _setup_logger(self) -> logging.Logger:\n",
    "        logger = logging.getLogger(f\"{self.store_name}_scraper\")\n",
    "        logger.setLevel(logging.INFO)\n",
    "        if not logger.handlers:\n",
    "            ch = logging.StreamHandler()\n",
    "            ch.setLevel(logging.INFO)\n",
    "            formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "            ch.setFormatter(formatter)\n",
    "            logger.addHandler(ch)\n",
    "        return logger\n",
    "    \n",
    "    def _rate_limit(self, min_delay: float = 1.0, max_delay: float = 3.0):\n",
    "        time.sleep(random.uniform(min_delay, max_delay))\n",
    "    \n",
    "    def _clean_price(self, price_str: str) -> Optional[float]:\n",
    "        if not price_str: return None\n",
    "        try:\n",
    "            cleaned = str(price_str).replace('Rs.', '').replace('PKR', '').replace('Rs', '').replace(',', '').strip()\n",
    "            return float(cleaned)\n",
    "        except (ValueError, AttributeError): return None\n",
    "    \n",
    "    def _parse_unit_quantity(self, text: str) -> tuple[Optional[str], Optional[float]]:\n",
    "        if not text: return None, None\n",
    "        import re\n",
    "        patterns = [r'(\\d+\\.?\\d*)\\s*(kg|g|l|ml|piece|pcs|pack)', r'(\\d+\\.?\\d*)(kg|g|l|ml|piece|pcs|pack)']\n",
    "        for pattern in patterns:\n",
    "            match = re.search(pattern, text.lower())\n",
    "            if match: return match.group(2), float(match.group(1))\n",
    "        return None, None\n",
    "    \n",
    "    def save_to_csv(self, products: List[Dict], filename: Optional[str] = None):\n",
    "        if not products: return\n",
    "        if filename is None:\n",
    "            filename = f\"{self.store_name.lower().replace(' ', '_')}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "        filepath = self.output_dir / filename\n",
    "        with open(filepath, 'w', newline='', encoding='utf-8-sig') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=self.csv_headers)\n",
    "            writer.writeheader()\n",
    "            for product in products:\n",
    "                row = {header: product.get(header, None) for header in self.csv_headers}\n",
    "                writer.writerow(row)\n",
    "        print(f\"\\u2713 Saved {len(products)} products to {filepath}\")\n",
    "\n",
    "    def validate_product(self, product: Dict) -> bool: return bool(product.get('product_name') and product.get('price'))\n",
    "    \n",
    "    @abstractmethod\n",
    "    async def scrape(self) -> List[Dict]: pass\n",
    "    @abstractmethod\n",
    "    async def get_categories(self) -> List[Dict]: pass\n",
    "    \n",
    "    def create_product_dict(self, **kwargs) -> Dict:\n",
    "        d = {h: kwargs.get(h) for h in self.csv_headers}\n",
    "        d['store_name'] = self.store_name\n",
    "        d['last_updated'] = datetime.now().isoformat()\n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Rahim Store Scraper Logic\n",
    "import asyncio\n",
    "from playwright.async_api import async_playwright\n",
    "\n",
    "class RahimStoreScraper(BaseScraper):\n",
    "    def __init__(self, output_dir: str = \"data\"):\n",
    "        super().__init__(store_name=\"Rahim Store\", base_url=\"https://www.rahimstore.com/department\", output_dir=output_dir)\n",
    "        self.departments = ['001', '002', '003', '004', '005', '006']\n",
    "        self.playwright = None\n",
    "        self.browser = None\n",
    "        self.context = None\n",
    "\n",
    "    async def setup_browser(self):\n",
    "        self.playwright = await async_playwright().start()\n",
    "        self.browser = await self.playwright.chromium.launch(headless=True, args=['--no-sandbox', '--disable-dev-shm-usage'])\n",
    "        self.context = await self.browser.new_context(viewport={'width': 1920, 'height': 1080}, user_agent='Mozilla/5.0')\n",
    "    \n",
    "    async def close_browser(self):\n",
    "        if self.context: await self.context.close()\n",
    "        if self.browser: await self.browser.close()\n",
    "        if self.playwright: await self.playwright.stop()\n",
    "\n",
    "    async def wait_for_products(self, page):\n",
    "        try: await page.wait_for_selector('.item.img-hover-zoom--quick-zoom', timeout=15000); return True\n",
    "        except: return False\n",
    "\n",
    "    async def scrape_dept(self, dept_id: str) -> List[Dict]:\n",
    "        page = await self.context.new_page()\n",
    "        all_p = []\n",
    "        try:\n",
    "            await page.goto(f\"{self.base_url}/{dept_id}\", wait_until='domcontentloaded')\n",
    "            pg = 1\n",
    "            while pg <= 5: # Limit pages for demo\n",
    "                if not await self.wait_for_products(page): break\n",
    "                cards = await page.query_selector_all('.item.img-hover-zoom--quick-zoom')\n",
    "                for card in cards:\n",
    "                    n_el = await card.query_selector('a[style*=\"display:block\"]')\n",
    "                    p_el = await card.query_selector('strong')\n",
    "                    if n_el and p_el:\n",
    "                        n = await n_el.inner_text(); p_txt = await p_el.inner_text()\n",
    "                        u, q = self._parse_unit_quantity(n)\n",
    "                        prod = self.create_product_dict(product_name=n.strip(), price=self._clean_price(p_txt), url=self.base_url+await n_el.get_attribute('href'), category=f\"Dept {dept_id}\", unit=u, quantity=q)\n",
    "                        if self.validate_product(prod): all_p.append(prod)\n",
    "                # Next page\n",
    "                next_btn = await page.query_selector('a.page-link[aria-label=\"Next\"]')\n",
    "                if next_btn and not await next_btn.evaluate('(e)=>e.parentElement.classList.contains(\"disabled\")'):\n",
    "                    await next_btn.click(); await asyncio.sleep(3); pg += 1\n",
    "                else: break\n",
    "            return all_p\n",
    "        finally: await page.close()\n",
    "\n",
    "    async def scrape(self) -> List[Dict]:\n",
    "        all_p = []\n",
    "        await self.setup_browser()\n",
    "        try:\n",
    "            for d in self.departments[:3]: # Limit departments for demo\n",
    "                all_p.extend(await self.scrape_dept(d))\n",
    "                await asyncio.sleep(2)\n",
    "        finally: await self.close_browser()\n",
    "        return all_p\n",
    "\n",
    "    async def get_categories(self) -> List[Dict]: return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Execution\n",
    "scraper = RahimStoreScraper()\n",
    "products = await scraper.scrape()\n",
    "if products:\n",
    "    scraper.save_to_csv(products)\n",
    "import pandas as pd\n",
    "if products:\n",
    "    df = pd.DataFrame(products)\n",
    "    display(df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
