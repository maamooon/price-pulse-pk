{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metro Online Scraper\n",
    "This notebook scrapes product data from Metro Online Pakistan using Playwright.\n",
    "\n",
    "### Instructions:\n",
    "1. Run the **Setup Cell** to install dependencies and Playwright browser.\n",
    "2. Run the **Scraper Logic Cell** to define the classes.\n",
    "3. Run the **Execution Cell** to start scraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Setup Cell\n",
    "!pip install playwright nest_asyncio pandas\n",
    "!playwright install chromium\n",
    "import os\n",
    "os.makedirs('data', exist_ok=True)\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Base Infrastructure\n",
    "import csv\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional\n",
    "from abc import ABC, abstractmethod\n",
    "import time\n",
    "import random\n",
    "\n",
    "class BaseScraper(ABC):\n",
    "    def __init__(self, store_name: str, base_url: str, output_dir: str = \"data\"):\n",
    "        self.store_name = store_name\n",
    "        self.base_url = base_url\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.output_dir.mkdir(exist_ok=True)\n",
    "        self.logger = self._setup_logger()\n",
    "        self.csv_headers = [\n",
    "            'store_name', 'product_name', 'brand', 'category', 'subcategory',\n",
    "            'price', 'discounted_price', 'unit', 'quantity', 'url', 'image_url', 'last_updated'\n",
    "        ]\n",
    "    \n",
    "    def _setup_logger(self) -> logging.Logger:\n",
    "        logger = logging.getLogger(f\"{self.store_name}_scraper\")\n",
    "        logger.setLevel(logging.INFO)\n",
    "        if not logger.handlers:\n",
    "            ch = logging.StreamHandler()\n",
    "            ch.setLevel(logging.INFO)\n",
    "            formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "            ch.setFormatter(formatter)\n",
    "            logger.addHandler(ch)\n",
    "        return logger\n",
    "    \n",
    "    def _rate_limit(self, min_delay: float = 1.0, max_delay: float = 3.0):\n",
    "        time.sleep(random.uniform(min_delay, max_delay))\n",
    "    \n",
    "    def _clean_price(self, price_str: str) -> Optional[float]:\n",
    "        if not price_str: return None\n",
    "        try:\n",
    "            cleaned = str(price_str).replace('Rs.', '').replace('PKR', '').replace('Rs', '').replace(',', '').strip()\n",
    "            return float(cleaned)\n",
    "        except (ValueError, AttributeError): return None\n",
    "    \n",
    "    def _parse_unit_quantity(self, text: str) -> tuple[Optional[str], Optional[float]]:\n",
    "        if not text: return None, None\n",
    "        import re\n",
    "        patterns = [r'(\\d+\\.?\\d*)\\s*(kg|g|l|ml|piece|pcs|pack)', r'(\\d+\\.?\\d*)(kg|g|l|ml|piece|pcs|pack)']\n",
    "        for pattern in patterns:\n",
    "            match = re.search(pattern, text.lower())\n",
    "            if match: return match.group(2), float(match.group(1))\n",
    "        return None, None\n",
    "    \n",
    "    def save_to_csv(self, products: List[Dict], filename: Optional[str] = None):\n",
    "        if not products: return\n",
    "        if filename is None:\n",
    "            filename = f\"{self.store_name.lower().replace(' ', '_')}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "        filepath = self.output_dir / filename\n",
    "        with open(filepath, 'w', newline='', encoding='utf-8-sig') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=self.csv_headers)\n",
    "            writer.writeheader()\n",
    "            for product in products:\n",
    "                row = {header: product.get(header, None) for header in self.csv_headers}\n",
    "                writer.writerow(row)\n",
    "        print(f\"\\u2713 Saved {len(products)} products to {filepath}\")\n",
    "\n",
    "    def validate_product(self, product: Dict) -> bool: return bool(product.get('product_name') and product.get('price'))\n",
    "    \n",
    "    @abstractmethod\n",
    "    async def scrape(self) -> List[Dict]: pass\n",
    "    @abstractmethod\n",
    "    async def get_categories(self) -> List[Dict]: pass\n",
    "    \n",
    "    def create_product_dict(self, **kwargs) -> Dict:\n",
    "        d = {h: kwargs.get(h) for h in self.csv_headers}\n",
    "        d['store_name'] = self.store_name\n",
    "        d['last_updated'] = datetime.now().isoformat()\n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Metro Scraper Logic\n",
    "import asyncio\n",
    "from playwright.async_api import async_playwright\n",
    "\n",
    "class MetroScraper(BaseScraper):\n",
    "    def __init__(self, output_dir: str = \"data\"):\n",
    "        super().__init__(store_name=\"Metro\", base_url=\"https://www.metro-online.pk\", output_dir=output_dir)\n",
    "        self.playwright = None\n",
    "        self.browser = None\n",
    "        self.context = None\n",
    "    \n",
    "    async def setup_browser(self):\n",
    "        self.playwright = await async_playwright().start()\n",
    "        self.browser = await self.playwright.chromium.launch(headless=True, args=['--no-sandbox', '--disable-dev-shm-usage'])\n",
    "        self.context = await self.browser.new_context(viewport={'width': 1920, 'height': 1080}, user_agent='Mozilla/5.0')\n",
    "    \n",
    "    async def close_browser(self):\n",
    "        if self.context: await self.context.close()\n",
    "        if self.browser: await self.browser.close()\n",
    "        if self.playwright: await self.playwright.stop()\n",
    "    \n",
    "    async def scroll_to_load_all_products(self, page):\n",
    "        prev_h = 0\n",
    "        attempts = 0\n",
    "        while attempts < 15:\n",
    "            curr_h = await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight); document.body.scrollHeight\")\n",
    "            await asyncio.sleep(2)\n",
    "            # Try load more\n",
    "            for sel in ['button:has-text(\\\"Load More\\\")', 'button:has-text(\\\"Show More\\\")']:\n",
    "                btn = await page.query_selector(sel)\n",
    "                if btn and await btn.is_visible():\n",
    "                    await btn.click()\n",
    "                    await asyncio.sleep(3)\n",
    "            if curr_h == prev_h: break\n",
    "            prev_h = curr_h\n",
    "            attempts += 1\n",
    "\n",
    "    async def get_categories(self) -> List[Dict]:\n",
    "        page = await self.context.new_page()\n",
    "        await page.goto(f\"{self.base_url}/home\", wait_until='networkidle')\n",
    "        await page.wait_for_selector('.CategoryGrid_grid_item__FXimL', timeout=15000)\n",
    "        res = await page.evaluate('''() => {\n",
    "            return Array.from(document.querySelectorAll('.CategoryGrid_grid_item__FXimL')).map(el => {\n",
    "                const l = el.querySelector('a');\n",
    "                const i = el.querySelector('img');\n",
    "                return { name: i?.alt || 'No name', url: l?.href || 'No URL' };\n",
    "            });\n",
    "        }''')\n",
    "        await page.close()\n",
    "        return res\n",
    "\n",
    "    async def get_subcategories(self, cat: Dict) -> List[Dict]:\n",
    "        page = await self.context.new_page()\n",
    "        try:\n",
    "            await page.goto(cat['url'], wait_until='networkidle')\n",
    "            sub_sel = '.sc-gKPRtg.jJzJeK'\n",
    "            try: await page.wait_for_selector(sub_sel, timeout=5000)\n",
    "            except: return [{'name': cat['name'], 'url': cat['url'], 'main_category': cat['name']}]\n",
    "            subs = await page.evaluate('''(sel) => {\n",
    "                return Array.from(document.querySelectorAll(sel + \" a\")).map(l => ({\n",
    "                    name: l.querySelector(\"h6, .sc-cwSeag\")?.textContent?.trim(),\n",
    "                    url: l.href\n",
    "                }));\n",
    "            }''', sub_sel)\n",
    "            for s in subs: s['main_category'] = cat['name']\n",
    "            return subs\n",
    "        finally: await page.close()\n",
    "\n",
    "    async def scrape_subcategory(self, sub: Dict) -> List[Dict]:\n",
    "        page = await self.context.new_page()\n",
    "        products = []\n",
    "        try:\n",
    "            await page.goto(sub['url'], wait_until='networkidle')\n",
    "            try: await page.wait_for_selector('.CategoryGrid_product_card__FUMXW', timeout=10000)\n",
    "            except: return []\n",
    "            await self.scroll_to_load_all_products(page)\n",
    "            cards = await page.evaluate('''() => {\n",
    "                return Array.from(document.querySelectorAll('.CategoryGrid_product_card__FUMXW')).map(el => ({\n",
    "                    name: el.querySelector('.CategoryGrid_product_name__3nYsN')?.textContent?.trim(),\n",
    "                    price: el.querySelector('.CategoryGrid_product_price__Svf8T')?.textContent?.trim(),\n",
    "                    url: el.querySelector('a')?.href,\n",
    "                    img: el.querySelector('img')?.src\n",
    "                }));\n",
    "            }''')\n",
    "            for c in cards:\n",
    "                u, q = self._parse_unit_quantity(c['name'])\n",
    "                p = self.create_product_dict(product_name=c['name'], price=self._clean_price(c['price']), url=c['url'], image_url=c['img'], category=sub['main_category'], subcategory=sub['name'], unit=u, quantity=q)\n",
    "                if self.validate_product(p): products.append(p)\n",
    "            return products\n",
    "        finally: await page.close()\n",
    "\n",
    "    async def scrape(self) -> List[Dict]:\n",
    "        all_p = []\n",
    "        await self.setup_browser()\n",
    "        try:\n",
    "            cats = await self.get_categories()\n",
    "            for cat in cats[:5]: # Limit for demo\n",
    "                subs = await self.get_subcategories(cat)\n",
    "                for sub in subs[:5]:\n",
    "                    all_p.extend(await self.scrape_subcategory(sub))\n",
    "                    await asyncio.sleep(1)\n",
    "        finally: await self.close_browser()\n",
    "        return all_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Execution\n",
    "scraper = MetroScraper()\n",
    "products = await scraper.scrape()\n",
    "if products:\n",
    "    scraper.save_to_csv(products)\n",
    "import pandas as pd\n",
    "if products:\n",
    "    df = pd.DataFrame(products)\n",
    "    display(df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
